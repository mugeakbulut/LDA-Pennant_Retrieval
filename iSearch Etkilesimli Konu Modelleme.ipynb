{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Konu Modelleme Görselleştirme Denemesi\n1. LDA ile konu modelleme\n1. Konu modellerini pyLDAvis ile görselleştirme\n1. LDA sonuçlarını t-SNE ([Bokeh Küütphanesi](https://bokeh.org)) ile görselleştirme","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%pylab inline\n\nimport pandas as pd\nimport pickle as pk\nfrom scipy import sparse as sp\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Abstractlar üzerinde çalışıyorum yine. Ama bu sefer iterasyon, ön işlemler falan çok olduğu için 1000 makale üzerinde deniyorum.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"p_df = pd.read_excel('http://mugeakbulut.com/phd/1000.xlsx',\n\n                     \n#p_df = pd.read_excel('http://mugeakbulut.com/phd/iSearch_full.xlsx',\n                    header=0,\n                    index_col=False,\n                    keep_default_na=True\n                  )\n\ndocs = array(p_df['Abstract'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Makale Pre-process ve vector haline getirme","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import RegexpTokenizer\n\ndef docs_preprocessor(docs):\n    tokenizer = RegexpTokenizer(r'\\w+')\n    for idx in range(len(docs)):\n        docs[idx] = docs[idx].lower()  # Hepsini küçük harfe çevir\n        docs[idx] = tokenizer.tokenize(docs[idx])  # Kelimelere böl\n\n    # Sayıları kaldır. Ama sası içerek kelimeleri değil. Fizik bu belli olmaz. \n    docs = [[token for token in doc if not token.isdigit()] for doc in docs]\n    \n    # Sadece bir karakter olanlar uçsun\n    docs = [[token for token in doc if len(token) > 3] for doc in docs]\n    \n    # Makalelerdeki tüm kelimeleri lematize et\n    lemmatizer = WordNetLemmatizer()\n    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n  \n    return docs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"docs = docs_preprocessor(docs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Bigram/trigram hesaplama","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import Phrases\n# bigram ve trigram (sadece 10 kez veya daha sık geçenler için).\nbigram = Phrases(docs, min_count=10)\ntrigram = Phrases(bigram[docs])\n\nfor idx in range(len(docs)):\n    for token in bigram[docs[idx]]:\n        if '_' in token:\n            # Token = bigram, makaleye ekle\n            docs[idx].append(token)\n    for token in trigram[docs[idx]]:\n        if '_' in token:\n            # Token = trigram, makaleye ekle\n            docs[idx].append(token)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Rare ve common tokens sil","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.corpora import Dictionary\n\n# Makalelerin sözlük temsilini oluştur\ndictionary = Dictionary(docs)\nprint('Initial makalelerdeki tekil kelime sayısı:', len(dictionary))\n\n# 10 belgeden az geçen kelimeleri veya makalelerin % 20'sinden fazlasında geçenleri at.\ndictionary.filter_extremes(no_below=10, no_above=0.2)\nprint('Rare ve common wordler silindikten sonra tekil kelime sayısı:', len(dictionary))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Budama**\n\nÖnce her bir dokümanın kelimelerden oluşan bir temsilini elde et","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = [dictionary.doc2bow(doc) for doc in docs]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Tekil token sayısı: %d' % len(dictionary))\nprint('Makale sayısı: %d' % len(corpus))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train LDA model...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import LdaModel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set training parameters.\nnum_topics = 4\nchunksize = 500 # size of the doc looked at every pass. Bunu biraz abartmış olaiblirim.\npasses = 20 # number of passes through documents\niterations = 400\neval_every = 1  # Böyle iyi. Öteki türlü çok zaman alıyor\n\n\ntemp = dictionary[0]  # sözlük load\nid2word = dictionary.id2token\n\n%time model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n                       alpha='auto', eta='auto', \\\n                       iterations=iterations, num_topics=num_topics, \\\n                       passes=passes, eval_every=eval_every)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pyLDAvis.gensim\npyLDAvis.enable_notebook()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pyLDAvis.gensim.prepare(model, corpus, dictionary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n\np_df['tokenz'] = docs\n\ndocs1 = p_df['tokenz'].apply(lambda l: l[:int0(len(l)/2)])\ndocs2 = p_df['tokenz'].apply(lambda l: l[int0(len(l)/2):])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus1 = [dictionary.doc2bow(doc) for doc in docs1]\ncorpus2 = [dictionary.doc2bow(doc) for doc in docs2]\n\n# LDA model dönüşümü kullan\nlda_corpus1 = model[corpus1]\nlda_corpus2 = model[corpus2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import OrderedDict\ndef get_doc_topic_dist(model, corpus, kwords=False):\n    \n#LDA dönüşümü, her makale için yalnızca sıfır olmayan ağırlıktaki konuları döndürüyor.\n#Bu işlev \"docs in topic\" matris dönüşümünü sağlıyor.\n\n    top_dist =[]\n    keys = []\n\n    for d in corpus:\n        tmp = {i:0 for i in range(num_topics)}\n        tmp.update(dict(model[d]))\n        vals = list(OrderedDict(tmp).values())\n        top_dist += [array(vals)]\n        if kwords:\n            keys += [array(vals).argmax()]\n\n    return array(top_dist), keys","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_dist1, _ = get_doc_topic_dist(model, lda_corpus1)\ntop_dist2, _ = get_doc_topic_dist(model, lda_corpus2)\n\nprint(\"Intra similarity: cosine similarity for corresponding parts of a doc(higher is better):\")\nprint(mean([cosine_similarity(c1.reshape(1, -1), c2.reshape(1, -1))[0][0] for c1,c2 in zip(top_dist1, top_dist2)]))\n\nrandom_pairs = np.random.randint(0, len(p_df['Abstract']), size=(400, 2))\n\nprint(\"Inter similarity: cosine similarity between random parts (lower is better):\")\nprint(np.mean([cosine_similarity(top_dist1[i[0]].reshape(1, -1), top_dist2[i[1]].reshape(1, -1)) for i in random_pairs]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def explore_topic(lda_model, topic_number, topn, output=True):\n    \"\"\"\n Top n kelimenin biçimlendirilmiş listesi\n    \"\"\"\n    terms = []\n    for term, frequency in lda_model.show_topic(topic_number, topn=topn):\n        terms += [term]\n        if output:\n            print(u'{:20} {:.3f}'.format(term, round(frequency, 3)))\n    \n    return terms","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_summaries = []\nprint(u'{:20} {}'.format(u'term', u'frequency') + u'\\n')\nfor i in range(num_topics):\n    print('Konu '+str(i)+' |---------------------\\n')\n    tmp = explore_topic(model,topic_number=i, topn=10, output=True )\n#     print tmp[:5]\n    topic_summaries += [tmp[:5]]\n    print","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#centroid termler yazılabilir\ntop_labels = {0: 'Şu an', 1:'Tamamen', 2:'Uyduruyorum', 3:'Uydurdum'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport nltk\n\nfrom nltk.corpus import stopwords\n\nstops = set(stopwords.words('english'))\n\ndef paper_to_wordlist( paper, remove_stopwords=True ):\n    '''\n    Metni bir kelime dizisine dönüştür. Sonra da listesini döndürür.\n    '''\n    lemmatizer = WordNetLemmatizer()\n    # 1. Remove non-letters\n    paper_text = re.sub(\"[^a-zA-Z]\",\" \", paper)\n    # 2. Convert words to lower case and split them\n    words = paper_text.lower().split()\n    # 3. Remove stop words\n    words = [w for w in words if not w in stops]\n    # 4. Remove short words\n    words = [t for t in words if len(t) > 2]\n    # 5. lemmatizing\n    words = [nltk.stem.WordNetLemmatizer().lemmatize(t) for t in words]\n\n    return(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntvectorizer = TfidfVectorizer(input='content', analyzer = 'word', lowercase=True, stop_words='english',\\\n                                  tokenizer=paper_to_wordlist, ngram_range=(1, 3), min_df=40, max_df=0.20,\\\n                                  norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtm = tvectorizer.fit_transform(p_df['Abstract']).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_dist =[]\nfor d in corpus:\n    tmp = {i:0 for i in range(num_topics)}\n    tmp.update(dict(model[d]))\n    vals = list(OrderedDict(tmp).values())\n    top_dist += [array(vals)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_dist, lda_keys= get_doc_topic_dist(model, corpus, True)\nfeatures = tvectorizer.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_ws = []\nfor n in range(len(dtm)):\n    inds = int0(argsort(dtm[n])[::-1][:4])\n    tmp = [features[i] for i in inds]\n    \n    top_ws += [' '.join(tmp)]\n    \np_df['Text_Rep'] = pd.DataFrame(top_ws)\np_df['clusters'] = pd.DataFrame(lda_keys)\np_df['clusters'].fillna(10, inplace=True)\n\ncluster_colors = {0: 'blue', 1: 'green', 2: 'yellow', 3: 'red', 4: 'skyblue', 5:'salmon', 6:'orange', 7:'maroon', 8:'crimson', 9:'black', 10:'gray'}\n\np_df['colors'] = p_df['clusters'].apply(lambda l: cluster_colors[l])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\ntsne = TSNE(n_components=2)\nX_tsne = tsne.fit_transform(top_dist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p_df['X_tsne'] =X_tsne[:, 0]\np_df['Y_tsne'] =X_tsne[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from bokeh.plotting import figure, show, output_notebook, save#, output_file\nfrom bokeh.models import HoverTool, value, LabelSet, Legend, ColumnDataSource\noutput_notebook()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"source = ColumnDataSource(dict(\n    x=p_df['X_tsne'],\n    y=p_df['Y_tsne'],\n    color=p_df['colors'],\n    label=p_df['clusters'].apply(lambda l: top_labels[l]),\n#     msize= p_df['marker_size'],\n    topic_key= p_df['clusters'],\n    title= p_df[u'Title'],\n    content = p_df['Text_Rep']\n))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = 'Konuların T-SNE görselleştirmesi'\n\nplot_lda = figure(plot_width=1000, plot_height=600, title=title,\n                  tools=\"pan,wheel_zoom,box_zoom,reset,hover\", \n                  x_axis_type=None, y_axis_type=None,\n                  min_border=1)\n\nplot_lda.scatter(x='x', y='y', legend='label', source=source,\n                 color='color', alpha=0.8, size=10)#'msize', )\n\n# hover toollar\nhover = plot_lda.select(dict(type=HoverTool))\nhover.tooltips = {\"Bilgi\": \"Başlık: @title, Anahtar Kelimeler: @content - Konu: @topic_key \"}\nplot_lda.legend.location = \"top_left\"\n\nshow(plot_lda)\n\n\n# save(plot_lda, '{}.html'.format(title))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}