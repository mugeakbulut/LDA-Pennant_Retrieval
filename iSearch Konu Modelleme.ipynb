{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Konu modelleme uygulaması Daniel Wolffram'ın (https://www.kaggle.com/danielwolffram) çalışmalarından uyarlanmıştır. \n",
    "\n",
    "* Derlem olarak Lykke ve arkadaşları (2010) tarafından oluşturulan iSearch tercih edilmiştir. Derlem 434.817 (PF + PN) belge, 65 konu listesi, her bir konu için 200 derecelendirilmiş ilgililik değerlendirmeleri ve 3,7 milyondan fazla dahili referanstan oluşmaktadır. ArXiv.org'dan alınan tam metin ve üstveri setlerinden meydana gelmektedir.\n",
    "\n",
    "Kaynaklar: \n",
    "\n",
    "Kyberd, P. (2015). Explainer: what are fundamental particles? (No. PRESSCUT-H-2015-109).\n",
    "\n",
    "Lykke M., Larsen B., Lund H., Ingwersen P. (2010) Developing a Test Collection for the Evaluation of Integrated Search. In: Gurrin C. et al. (eds) Advances in Information Retrieval. ECIR 2010. Lecture Notes in Computer Science, vol 5993. Springer, Berlin, Heidelberg.\n",
    "> \n",
    "![](https://images.theconversation.com/files/75083/original/image-20150317-22294-qb87ml.jpg?ixlib=rb-1.1.0&q=30&auto=format&w=240&h=300&fit=crop&dpr=2)\n",
    "\n",
    "Spesifik olarak kuark (quark) terimini seçtim. Altı kuark türü bulunuyor:\n",
    "\n",
    "* yukarı (up)\n",
    "* aşağı (down)\n",
    "* tılsım (charm)\n",
    "* acayip (strange)\n",
    "* üst (top)\n",
    "* alt (bottom)\n",
    "\n",
    "Bu altı kuark da üç çifte ayrılıyor: \"yukarı ve aşağı\", “sevimli ve tuhaf\" ile “üst ve alt\" (önceden “gerçek (truth) ve güzel (beauty)). Aşağı ve yukarı kuarklar, her atomun kalbinde bulunan proton ve nötronları oluşturacak şekilde birleşirler. Sadece kuark çiftlerinin en hafifi olan aşağı ve yukarı kuarklar normal madde içinde bulunurlar. Sevimli/tuhaf ve üst/alt çiftlerinin şu andaki evrende oynadıkları bir rol yok gibi görülmektedir; ama ağırlığı daha fazla olan leptonlar gibi, evrenin ilk anlarında bizi meydana getiren evreni oluşturmada görev almışlardı. (https://evrimagaci.org/parcacik-fizigi-standart-model-nedir-temel-parcaciklar-nelerdir-3733)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paketleri Yükleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "from IPython.utils import io\n",
    "with io.capture_output() as captured:\n",
    "    !pip install scispacy\n",
    "    !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import scispacy\n",
    "import spacy\n",
    "import en_core_sci_lg\n",
    "\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "import joblib\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "from ipywidgets import interact, Layout, HBox, VBox, Box\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from tqdm import tqdm\n",
    "from os.path import isfile\n",
    "\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"dark_background\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verileri Yükleme ve Hazırlma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/mugeakbulut/Academia/Right Now/PhD_Tez/Programlar/Kaggle/iSearch.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b12d0055a754>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mkeep_default_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, io, engine)\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstringify_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Install xlrd >= 1.0.0 for Excel support\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xlrd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36mload_workbook\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/mugeakbulut/Academia/Right Now/PhD_Tez/Programlar/Kaggle/iSearch.xlsx'"
     ]
    }
   ],
   "source": [
    "#df = pd.read_excel('http://mugeakbulut.com/phd/dataset.xlsx',\n",
    "#df = pd.read_excel('http://mugeakbulut.com/phd/iSearch_full.xlsx',\n",
    "#df = pd.read_excel('http://mugeakbulut.com/phd/100.xlsx',\n",
    "encoding=\"ISO-8859-1\",\n",
    "df = pd.read_excel('/Users/mugeakbulut/Academia/Right Now/PhD_Tez/Programlar/Kaggle/iSearch.xlsx',\n",
    "header=0,\n",
    "index_col=False,\n",
    "keep_default_na=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kaç tane listelesin?\n",
    "df.head(n = 50) # df.head(2) = df.head(n = 50)\n",
    "\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataframe'in (df) satır ve sütun sayısı nedir?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://)Abstractlar üzerinde çalışıyorum. DİKKAT! İndirme tamamlanınca UTF-8 yapıp \"\\n\"leri kaldıracağım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = df.Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example snippet\n",
    "all_texts[0][:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA (Latend Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ön işleme için [scispaCy](https://allenai.github.io/scispacy/) paketini kullandım.\n",
    "https://www.kaggle.com/alexandruuu/spacy-preprocessing güzel anlatıyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medium model\n",
    "nlp = en_core_sci_lg.load(disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "nlp.max_length = 2000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "    return [word.lemma_ for word in nlp(sentence) if not (word.like_num or word.is_stop or word.is_punct or word.is_space or len(word)==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New stop words list \n",
    "customize_stop_words = [\n",
    "    'doi', '\\n', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n",
    "    'rights', 'reserved', 'result','permission', 'used', 'using', 'license', 'fig', 'ArXiv', 'fig.', 'al.', 'Elsevier', 'PMC', 'CZI',\n",
    "    '-PRON-'\n",
    "]\n",
    "\n",
    "# Mark them as stop words\n",
    "for w in customize_stop_words:\n",
    "    nlp.vocab[w].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../input/topic-modeling-finding-related-articles/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dosyaları oluşturma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, min_df=2)\n",
    "data_vectorized = vectorizer.fit_transform(tqdm(all_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, max_features=800000)\n",
    "# data_vectorized = vectorizer.fit_transform(tqdm(all_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# data_vectorized.shape # with bigrams: 6428134\n",
    "\n",
    "# data_vectorized.shape # all 1.2 mio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en sık geçen kelimeler\n",
    "word_count = pd.DataFrame({'word': vectorizer.get_feature_names(), 'count': np.asarray(data_vectorized.sum(axis=0))[0]})\n",
    "\n",
    "word_count.sort_values('count', ascending=False).set_index('word')[:20].sort_values('count', ascending=True).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joblib.dump(vectorizer, 'vectorizer.csv')\n",
    "joblib.dump(data_vectorized, 'data_vectorized.csv')\n",
    "joblib.dump(vectorizer, 'vectorizer.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# if not (isfile(filepath + 'vectorizer.csv') & isfile(filepath + 'data_vectorized.csv')):\n",
    "#     print('Files not there: generating')\n",
    "#     vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, max_features=800000)\n",
    "#     data_vectorized = vectorizer.fit_transform(tqdm(all_texts))\n",
    "#     joblib.dump(vectorizer, 'vectorizer.csv')\n",
    "#     joblib.dump(data_vectorized, 'data_vectorized.csv')\n",
    "\n",
    "# else:\n",
    "#     vectorizer = joblib.load(filepath + 'vectorizer.csv')\n",
    "#     data_vectorized = joblib.load(filepath + 'data_vectorized.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaç Konu Olsun?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=20, random_state=0)\n",
    "lda.fit(data_vectorized)\n",
    "joblib.dump(lda, 'lda.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# # Train/Load Model\n",
    "# if not (isfile(filepath + 'lda.csv')):\n",
    "#     print('File not there: generating')\n",
    "#     lda = LatentDirichletAllocation(n_components=50, random_state=0)\n",
    "#     lda.fit(data_vectorized)\n",
    "\n",
    "#     joblib.dump(lda, 'lda.csv')\n",
    "\n",
    "# else:\n",
    "#     lda = joblib.load(filepath + 'lda.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konuları Belirleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, vectorizer, n_top_words):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"\\nTopic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "print_top_words(lda, vectorizer, n_top_words=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each article is a mixture of topics / a distribution over topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_dist = pd.DataFrame(lda.transform(data_vectorized))\n",
    "doc_topic_dist.to_csv('doc_topic_dist.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "#if not (isfile(filepath + 'doc_topic_dist.csv')):\n",
    "#       print('File not there: generating')\n",
    "#        doc_topic_dist = pd.DataFrame(lda.transform(data_vectorized))\n",
    "#         doc_topic_dist.to_csv('doc_topic_dist.csv', index=False)\n",
    "# else:\n",
    "#        doc_topic_dist = pd.read_csv(filepath + 'doc_topic_dist.csv')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Makaleler (in Topic Space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "#50 tane sıralasın\n",
    "#doc_topic_dist.head(n = 50) # df.head(2) = df.head(n = 50)\n",
    "\n",
    "doc_topic_dist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def get_k_nearest_docs(doc_dist, k=10, lower=1950, upper=2020, only_quark=False, get_dist=False):\n",
    "    '''\n",
    "    doc_dist: topic distribution (sums to 1) of one article\n",
    "    \n",
    "    Returns the index of the k nearest articles (as by Jensen–Shannon divergence in topic space). \n",
    "    '''\n",
    "    \n",
    "    relevant_time = df['Year '].between(lower, upper)\n",
    "    \n",
    "    if only_quark:\n",
    "        temp = doc_topic_dist[relevant_time & is_quark_article]\n",
    "        \n",
    "    else:\n",
    "        temp = doc_topic_dist[relevant_time]\n",
    "         \n",
    "    distances = temp.apply(lambda x: jensenshannon(x, doc_dist), axis=1)\n",
    "    k_nearest = distances[distances != 0].nsmallest(n=k).index\n",
    "    \n",
    "    if get_dist:\n",
    "        k_distances = distances[distances != 0].nsmallest(n=k)\n",
    "        return k_nearest, k_distances\n",
    "    else:\n",
    "        return k_nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "#d = get_k_nearest_docs(doc_topic_dist[df.paper_id == 'PN018446'].iloc[0])\n",
    "\n",
    "#sb.kdeplot(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "def plot_article_dna(paper_id, width=20):\n",
    "    t = df[df.DOCNO == paper_id].title.values[0]\n",
    "    doc_topic_dist[df.DOCNO == paper_id].T.plot(kind='bar', legend=None, title=t, figsize=(width, 4))\n",
    "    plt.xlabel('Topic')\n",
    "\n",
    "def compare_dnas(paper_id, recommendation_id, width=20):\n",
    "    t = df[df.DOCNO == recommendation_id].Title.values[0]\n",
    "    temp = doc_topic_dist[df.DOCNO == paper_id]\n",
    "    ymax = temp.max(axis=1).values[0]*1.25\n",
    "    temp = pd.concat([temp, doc_topic_dist[df.DOCNO == recommendation_id]])\n",
    "    temp.T.plot(kind='bar', title=t, figsize=(width, 4), ylim= [0, ymax])\n",
    "    plt.xlabel('Topic')\n",
    "    plt.legend(['Selection', 'Recommendation'])\n",
    "\n",
    "# compare_dnas('90b5ecf991032f3918ad43b252e17d1171b4ea63', 'a137eb51461b4a4ed3980aa5b9cb2f2c1cf0292a')\n",
    "\n",
    "def dna_tabs(paper_ids):\n",
    "    k = len(paper_ids)\n",
    "    outs = [widgets.Output() for i in range(k)]\n",
    "\n",
    "    tab = widgets.Tab(children = outs)\n",
    "    tab_titles = ['Makale ' + str(i+1) for i in range(k)]\n",
    "    for i, t in enumerate(tab_titles):\n",
    "        tab.set_title(i, t)\n",
    "    display(tab)\n",
    "\n",
    "    for i, t in enumerate(tab_titles):\n",
    "        with outs[i]:\n",
    "            ax = plot_article_dna(paper_ids[i])\n",
    "            plt.show(ax)\n",
    "\n",
    "def compare_tabs(paper_id, recommendation_ids):\n",
    "    k = len(recommendation_ids)\n",
    "    outs = [widgets.Output() for i in range(k)]\n",
    "\n",
    "    tab = widgets.Tab(children = outs)\n",
    "    tab_titles = ['Makale ' + str(i+1) for i in range(k)]\n",
    "    for i, t in enumerate(tab_titles):\n",
    "        tab.set_title(i, t)\n",
    "    display(tab)\n",
    "\n",
    "    for i, t in enumerate(tab_titles):\n",
    "        with outs[i]:\n",
    "            ax = compare_dnas(paper_id, recommendation_ids[i])\n",
    "            plt.show(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seçilen Makaleyle İlgili Makaleleri Sıralama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benzerlik ölçüsü olarak **Jensen-Shannon distance** kullanıldı. Jensen-Shannon mesafesi iki olasılık dağılımı arasındaki mesafeyi ölçmek için kullanılıyor. Information radius (yani iRad) veya ortalamaya olan toplam uzaklık (total divergence to average) olarak da geçiyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def recommendation(paper_id, k=5, lower=1950, upper=2020, only_quark=False, plot_dna=False):\n",
    "    '''\n",
    "    Returns the title of the k papers that are closest (topic-wise) to the paper given by paper_id.\n",
    "    '''\n",
    "    \n",
    "    print(df.Title[df.DOCNO == paper_id].values[0])\n",
    "\n",
    "    recommended, dist = get_k_nearest_docs(doc_topic_dist[df.DOCNO == paper_id].iloc[0], k, lower, upper, only_quark, get_dist=True)\n",
    "    recommended = df.iloc[recommended].copy()\n",
    "    recommended['similarity'] = 1 - dist \n",
    "    \n",
    "    h = '<br/>'.join(['<a href=\"' + l + '\" target=\"_blank\">'+ n + '</a>' +' (Benzerlik: ' + \"{:.2f}\".format(s) + ')' for l, n, s in recommended[['Link','Title', 'similarity']].values])\n",
    "    display(HTML(h))\n",
    "    \n",
    "    if plot_dna:\n",
    "        compare_tabs(paper_id, recommended.DOCNO.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "recommendation('PN018464', k=5, plot_dna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "recommendation('PN018548', k=10, plot_dna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "recommendation('PN018456', k=1, plot_dna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "recommendation('PN018444', k=5, only_quark=False, plot_dna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "recommendation('PN018501', k=10, plot_dna=True)#PN018501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Widget: Quark ile ilgili olanlar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes it easier to pick a paper (you don't have to search the paper_id)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "def related_papers():\n",
    "    '''\n",
    "    Creates a widget where you can select one of many papers about quarks and then displays related articles from the whole dataset.\n",
    "    '''\n",
    "    justquark_papers = df[df.Abstract.str.contains('quark|top quark|up quark|charm quark|strange quark')][['DOCNO', 'Title']] # are there more names?\n",
    "    title_to_id = justquark_papers.set_index('Title')['DOCNO'].to_dict()\n",
    "    \n",
    "    def main_function(bullet, k=5, year_range=[1950, 2020], only_quark=False):\n",
    "        recommendation(title_to_id[bullet], k, lower=year_range[0], upper=year_range[1], only_quark=only_quark)\n",
    "    \n",
    "    yearW = widgets.IntRangeSlider(min=1950, max=2020, value=[2010, 2020], description='Year Range', \n",
    "                                   continuous_update=False, layout=Layout(width='40%'))\n",
    "    justquarkW = widgets.Checkbox(value=False,description='Sadece quarklarla ilgili',disabled=False, indent=False, layout=Layout(width='20%'))\n",
    "    kWidget = widgets.IntSlider(value=10, description='k', max=50, min=1, layout=Layout(width='20%'))\n",
    "\n",
    "    bulletW = widgets.Select(options=title_to_id.keys(), layout=Layout(width='90%', height='200px'), description='Title:')\n",
    "\n",
    "    widget = widgets.interactive(main_function, bullet=bulletW, k=kWidget, year_range=yearW, only_quark=justquarkW)\n",
    "\n",
    "    controls = VBox([Box(children=[widget.children[:-1][1], widget.children[:-1][2], widget.children[:-1][3]], \n",
    "                         layout=Layout(justify_content='space-around')), widget.children[:-1][0]])\n",
    "    output = widget.children[-1]\n",
    "    display(VBox([controls, output]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_papers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Görevler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Görevleri konu ile eşleyip ilgili makaleleri sıralayalım"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "#taskID:iSearch'teki Topic no oluyor\n",
    "#task içinde \n",
    "#   1.satır: current_information_need\n",
    "#   2.satır: work_task\n",
    "#   3.satır: background_knowledge\n",
    "#   4.satır ideal_answer\n",
    "#   5.satır: search_terms\n",
    "#dolayısıyla 3. satır falan gereksiz.\n",
    "\n",
    "#author_id 85\n",
    "task1 = [\"I am looking for information about manipulation and immobilisation of nano spheres and peptide nano particles.\",\n",
    "\"I am starting my master thesis in which I will fabricate self-assembled peptide nano spheres, which needs to be manipulated and immobilized. This is intended done by filling them with metals e.g. gold (Au) or iron (Fe) and use the electrical and magnetic properties to manipulate and immobilise the spheres. This could be by using dielectrophoresis on a chip or micro fluidic device. The nano spheres are intended for biomedical use in which techniques for manipulating biological and biomedical materials are interesting.\",\n",
    "\"The background knowledge is limited since the thesis is starting up this week. But I have been working with sorting of blood cells in micro fluidic devices and flow cytometry.\",\n",
    "\"An ideal answer could be an article showing how to manipulate peptide nano spheres. But in it would in fact might be better if there isnt any articles on the subject since this would mean the research is new.\",\n",
    "\"Manipulation, nano spheres, peptides, immobilisation\"]\n",
    "\n",
    "#author_id 85\n",
    "task2 = ['I am looking for information about manipulation and sorting of magnetic particles, beads or spheres on nanoscale. This might be in a micro fluidic system.',\n",
    "'As a part of my master thesis it is interesting to fabricate a sorting device which can sort magnetic nano spheres from a sample. This will often be in a micro fluidic device because the nano sphere/particles often will be diluted in some sort of solution.',\n",
    "'I have been making sorting devices for micro particles based on flow profiles in a microfluidic system.',\n",
    "'Published material on how to sort magnetic beads, particles or spheres on nanoscale.',\n",
    "'Nano spheres, beads, magnetic, sorting']\n",
    "\n",
    "#task = ['',\n",
    "#'',\n",
    "#'',\n",
    "#'',\n",
    "#'']\n",
    "\n",
    "tasks={'I am looking for information about manipulation and immobilisation of nano spheres and peptide nano particles': task1,\n",
    "       'I am looking for information about manipulation and sorting of magnetic particles, beads or spheres on nanoscale. This might be in a micro fluidic system.': task2} \n",
    "       #'I am looking for information on how to make an on chip flow cytometry using an LED as a light source and an APD (Avalanche photodiode) as a detector.': task3, \n",
    "       #'I want information about protein-protein interaction, the surface charge distribution of these proteins and how this has been investigated with Electrostatic Force Microscopy (EFM). The proteins of interest are the Avidin-Biotin and IgG-anti-IgG systems.The most important for the scenario is how this has been investigated with EFM.': task4,\n",
    "       #'I would like some basic knowledge about two methods called Elisa and Immunoassay.': task5, \n",
    "       #'I would like some basic knowledge of the electrical properties of Avidin-Biotin and IgG - anti-IgG, the electrostatic behaviour, surface charge distribution and there interaction.': task6, \n",
    "       #'I have a certain equation that I would like to solve numerically. I need to find out what people have done previously with similar equations, preferably of the exact same form. It is a system of nonlinear coupled ordinary  integro-differential equations with delay terms. This means that the derivatives of the functions  to solve for depend on a combination of integrals, function values and derivatives of these functions, whose arguments (independent variables) can be smaller (hence the term \"delay\"). I am looking for theory on this type of equations or sufficiently similar types with emphasis on numerical solution and implementation.': task7,\n",
    "       #'Descriptions of models and theory concerning passive mode-locking in linear cavities. It is important that it is for linear and not ring cavities or some sort of coupled cavities or mode-locking using coupled external cavities.': task8, \n",
    "       #'I want information on how to measure dielectric properties on cells, example in microfluidic systems.': task9,\n",
    "       #'Literature on sorting of cells in microfludic systems. One example could be cancer cells.': task10,\n",
    "       #'Theory on dielectricphoresis for cell sorting.': task11,\n",
    "       #'Im looking for reaction kinetics of borate and phosphate buffers. More specifically, I need information on the forward and backward rate constants.': task12,\n",
    "       #'Im looking for information on possible ways to achieve significant slip-lengths in micro- and nanochannels.': task13,\n",
    "      # 'Im looking for the dynamics in induced-charge electro-osmotic flow with a finite electric double layer.': task14,\n",
    "      # 'I am looking for examples of intracellular recordings on cells. Especially Im looking for recordings of the intracellular electrical potential of living neurons. I want to investigate which methods that previously have been used to study the intracellular environment of living cells. And again especially methods that has been used to record the intracellular electrical potential': task15,\n",
    "       #'I am looking for a general expression for the heat transfer coefficient in a liquid solid interface. I want to incorporate this expression into a simulation as a boundary condition during a thermodynamic numerical simulation.': task16,\n",
    "       #'I am looking for a general theory of thermo pneumatic actuation. Especially I am interested in am overview over devices that utilize thermo pneumatic actuation. It would also be interesting to how this actuation principle has been incorporated into a fabrication sequence of a final device.': task17,\n",
    "      # 'I am looking for results for experimental data where researchers have built a vertical cavity surface emitting laser (VCSEL), deposited a chemical coating (an organic thin film polymer, e.g. MAH) on these and then monitored the power output, or voltage as function of exposure to a gaseous compound.': task18,\n",
    "       #'I am looking for all an extensive research on the articles that exist on making tunable vertical cavity surface emitting laser diodes (tunable VCSEL) by integration a micro-electromechanical system with a VCSEL.': task19,\n",
    "       #'I am looking for both theoretical and experimental articles describing the properties of single laser cavities in photonic crystals - The electromagnetic mode profiles, lasing curves, threshold powers and emission spectra.': task20,\n",
    "       #'I am looking for both theoretical and experimental articles describing the properties of coupled arrays of laser cavities in photonic crystals - The electromagnetic mode profiles, lasing curves, threshold powers, emission spectra and coupling strength.': task21,\n",
    "      # 'I am looking for a numerical method to calculate the far-field (or far-zone) emission spectrum from the electromagnetic modes from Finite Difference Time Domain (FDTD) calculations.': task22,\n",
    "       #'I am looking for research papers investigating the properties of coupled resonator optical waveguides (sometimes abbreviated CROW) or equivalently coupled resonators in photonic crystals. The papers can investigate active or passive structures, but in particular, I am interested in the optical steady state intensity transmission properties or general dynamical properties of the structures.': task23,\n",
    "       #'The effect of the Raman coefficient on the super continuum spectrum generated by soliton propagation in non-liner fibres, influenced by self-steepening or optical shock effect. In particular I look for results obtained by using a Fourier Split Step Method for solving the non-linear Schrvdinger equation.': task24,\n",
    "       #'An example of an implementation of Fast Fourier Split Step method, and in particular an example of how to use Fast Fourier Transformation (FFT) algorithms in Matlab (a commercial software package) correctly.': task25,\n",
    "       #'I want information about piezoelectric energy harvesting. I would like reviews and research articles about the topic vibrational energy harvesting with special emphasize on piezo electric conversion.  ': task26,\n",
    "       #'I want information about low resonance frequency cantilevers, membranes, micro structures and MEMS structures.': task27,\n",
    "       #'I want information about the application of non-equilibrium Greens functions to optical phenomena in semiconductors. Especially quantum dot systems with the interaction with phonons and photons.': task28,\n",
    "       #'I want information about single-photon indistinguishability in connection with semiconductor single-photon sources. Especially theoretical models describing the important physics while including non-Markovian effects arising from electron-phonon interactions.': task29,\n",
    "       #'I need data on wet oxidation of GaAs (galiumarsenide).': task30,\n",
    "       #'I need information on the piezoelectric coefficient of AlGaAs.': task31,\n",
    "       #'I am looking for information high-index-constrast (HCG) subwavelength grating mirrors.': task32,\n",
    "       #'I am looking for some description of the basic properties of the SO(N), SU(N), O(N), and U(N) groups and their generators.More specifically I need to know about the SU(2), the SU(3), the SU(4), and the SO(6) groups.Some introductory material on the associated Lie Algebras would be nice.': task33,\n",
    "       #'I am looking for derivations concerning the vertex structure and the correlation functions of N=4 Supersymmetric Yang-Mills Theory with a U(N) gauge group (often called SYM).More specifically I need information on derivations originating from the scalar interaction terms of the action of the theory.': task34,\n",
    "       #'A brief introduction to String Theory in an AdS background (which is short for Anti-de-Sitter space). Connections with this topic to the AdS/CFT-correspondence would be helpful, too. Specifically I am looking for details on quantization of the theory (though this has not been done successfully yet) and the holography and the properties of the AdS manifold.': task35,\n",
    "       #'I want information about atmospheric disturbances of light waves and strehl ratio.': task36,\n",
    "       #'I want information about techniques to calculate and subtract bias, flatfield, darkcurrent and other sources of error from CCDs.': task37,\n",
    "       #'I want information about Lucky Imaging and Speckle Imaging techniques using L3-CCD chips.': task38,\n",
    "       #'I  am looking for literature that can give me an overview on the research field symbolic dynamics and biological networks especially work concerning the Hastings-Powell (HP) model and the Blausius-Huppert-Stone (BHS) model. I want to find out, by varying the parameters in these two models, if they are ultimately identical except from one term. I think this has not been done before, but I am very interested in finding out if  the parameter space of these two models has been investigated by others. By this I mean, if anybody else has reported findings from varying the parameters of the Hastings-Powell model and/or the Blausius-Huppert-Stone model. The findings could for instance be: If  this parameter in the HP-model is assigned this value, the system goes chaotic - if the parameter is assigned this other value the system becomes periodic. I am interested in the parameter space where these two models are periodic.NB: There is uncertainty about the spelling of Blausius from the Blausius-Huppert-Stone model, sometimes he is also spelled Blasius. I do not know which of these is the correct spelling.': task39,\n",
    "       #'Effect of octanol and other primary alchohols on phase transition in 1,2-Dipalmitoyl-sn-Glycero-3-Phosphocholine (DPPC) and other phospholipids. Has any effect on transition enthalpy been reported? I wish to find out if the presence of alcohol affects the transition enthalpy of the main transition of DPPC and other phospholipids. The transition is also called melting and freezing.': task40,\n",
    "       #'How much energy can be generated globally from wind energy systems?This is related to the question of how much wind energy is dissipated globally.And equivalently, how much mechanical energy is dissipated in the ocean?What are the effects of large scale wind energy systems on the atmosphere?': task41,\n",
    "       #'Typical dates for melting and freezing of ice/snow in the polar region? Are polar sea ice and snow on land melting more and faster than before?I want to know what are the main points of debate within different scientific communities concerned with ice/snow melting or freezing in the polar region.I have access to new data and a new method that gives new information in a more consistent way. I want to relate this to more traditional studies.': task42,\n",
    "       #'The solar wind and its interaction with planetary magnetospheres as well as the interstellar medium, but with special focus in the shielding effects of the wind with respect to galactic cosmic rays from supernovae.': task43,\n",
    "       #'Models of emerging magnetic flux tubes.': task44,\n",
    "       #'Information about nano-structured anti-reflective surfaces.': task45,\n",
    "       #'Implementaion of diffractive optics on multi-processor systems, especially on graphics processing units (GPU) using the CUDA-framework.': task46,\n",
    "       #'I am looking for information on manufactoring of ZnO films by rf magnetron sputtering and specifically highly doped ZnO': task47,\n",
    "       #'Information on characterization by photo luminescence of highly doped ZnO films.': task48,\n",
    "       #'Information on characterization of resistivity of highly doped ZnO films.': task49,\n",
    "       #'I am looking for articles/books describing the production of RF magnetron sputter coated zinc oxide films doped with Al. The articles should include descriptions of the influence different parameters have on the film quality, especially the RF effect, the deposition rate, the amount of oxygen in the chamber, the sputter pressure, and the deposition temperature.': task50,\n",
    "       #'I am looking for articles/books describing the post annealing of RF magnetron sputter coated zinc oxide films. They should describe the electrical properties of the films and how they are changed due to the post annealing. ': task51,\n",
    "       #'I am looking for articles/books describing the characterisation of post annealed RF magnetron sputter coated zinc oxide film, possibly doped with Al. The characterisation should include; surface characterisation, electrical characterisation, impurity characterisation, and optically characterisation. ': task52,\n",
    "       #'The economy/financing of  the development of new transparent electrodes for use in solar cells.': task53,\n",
    "       #'I am looking for the radiative lifetime of erbium ions embedded in SiO2 deposited by magnetron sputtering': task54,\n",
    "       #'I am looking for experimental evidence for the energy transfer mechanism between silicon nanocrystals and erbium ions in a SiO2 matrix': task55,\n",
    "       #'I am looking for theoretical models of silicon nanocrystals preferably embedded in SiO2 and their optical properties': task56,\n",
    "       #'I would like to find work done on \"trions\" (also known as \"charged excitons\") especially with focus on trions in carbon nanotubes.': task57,\n",
    "       #'I am looking for Hartree-Fock models for trion or helium-like problems and extension to this kind of problems in terms of perturbation theory. Other kind of extensions would be great too.': task58,\n",
    "       #'I am looking for biexcitonic models for second order optical response in semiconductor.': task59,\n",
    "       #'An algorithm capable of performing decomposition of non-convex, triangulated polyhedrons (3-Dimensional polygons) into their convex parts.': task60,\n",
    "       #'Algorithms for efficient Ray-Tracing of non-convex polyhedrons. More specifically, Ray-Tracing of Constructive Solid Geometry (CSG) unions and differences of convex polyhedrons. ': task61,\n",
    "       #'An algorithm for calculating a triangulated minimal surface problem of 3D curves represented as polygons (stepwise linear approximation of curves).': task62,\n",
    "       #'An algorithm for automatic generation of Constructive Solid Geometry (CGS) trees by providing information about the convex and concave regions of a surface.': task63,\n",
    "       #'Information about fabrication and testing of field effect transistors (FETs) on silicon-on-insulator (SOI) wafers.': task64,\n",
    "       #'Finite difference simulations of the potential inside a metal on oxide field effect transistors, MOSFETs. Solving Poissons equation inside a MOSFET with the use of finite difference method.': task65}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "def relevant_articles(tasks, k=3, lower=1950, upper=2020, only_covid19=False):\n",
    "    tasks = [tasks] if type(tasks) is str else tasks \n",
    "    \n",
    "    tasks_vectorized = vectorizer.transform(tasks)\n",
    "    tasks_topic_dist = pd.DataFrame(lda.transform(tasks_vectorized))\n",
    "\n",
    "    for index, bullet in enumerate(tasks):\n",
    "        print(bullet)\n",
    "        recommended = get_k_nearest_docs(tasks_topic_dist.iloc[index], k, lower, upper, only_covid19)\n",
    "        recommended = df.iloc[recommended]\n",
    "        \n",
    "\n",
    "        h = '<br/>'.join(['<a href=\"' + l + '\" target=\"_blank\">'+ n + '</a>' for l, n in recommended[['Link','Title']].values])\n",
    "        display(HTML(h))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Görev 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "#task 1 için beşer tane sıralayalım\n",
    "relevant_articles(task1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Görev 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "relevant_articles(task2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Widget 2: Görevler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Belirli bir görevle ilgili çalışmaları sıralama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "def relevant_articles_for_task():\n",
    "    def main_function(bullet, task, k=5, year_range=[1950, 2020], only_quark=False):\n",
    "        relevant_articles([bullet], k, lower=year_range[0], upper=year_range[1], only_quark=only_quark)\n",
    "        bulletW.options = tasks[task]    \n",
    "\n",
    "    yearW = widgets.IntRangeSlider(min=1950, max=2020, value=[2010, 2020], description='Year Range', \n",
    "                                   continuous_update=False, layout=Layout(width='40%'))\n",
    "    quarkW = widgets.Checkbox(value=True,description='Only Quark-Papers',disabled=False, indent=False, layout=Layout(width='20%'))\n",
    "    kWidget = widgets.IntSlider(value=10, description='k', max=50, min=1, layout=Layout(width='30%'))\n",
    "\n",
    "    taskW = widgets.Dropdown(options=tasks.keys(), layout=Layout(width='90%', height='50px'), description='Task:')\n",
    "    init = taskW.value\n",
    "    bulletW = widgets.Select(options=tasks[init], layout=Layout(width='90%', height='200px'), description='Bullet Point:')\n",
    "\n",
    "    widget = widgets.interactive(main_function, task=taskW, bullet=bulletW, k=kWidget, year_range=yearW, only_quark=quarkcovidW)\n",
    "    \n",
    "    controls = VBox([HBox([widget.children[2], widget.children[3], widget.children[4]], layout=Layout(width='90%', justify_content='space-around')),\n",
    "                     widget.children[1],\n",
    "                     widget.children[0]], layout=Layout(align_items='center'))\n",
    "    \n",
    "    output = widget.children[-1]\n",
    "    display(VBox([controls, output]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_articles_for_task()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Widget: Free Text Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this widget you can insert any kind of text (abstract, paragraph, full text, keywords, questions, ...) and find related articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "def relevant_articles_for_text():    \n",
    "    textW = widgets.Textarea(\n",
    "        value='',\n",
    "        placeholder='Type something',\n",
    "        description='',\n",
    "        disabled=False,\n",
    "        layout=Layout(width='90%', height='200px')\n",
    "    )\n",
    "\n",
    "    yearW = widgets.IntRangeSlider(min=1950, max=2020, value=[2010, 2020], description='Year Range', \n",
    "                               continuous_update=False, layout=Layout(width='40%'))\n",
    "    quarkW = widgets.Checkbox(value=True,description='Only quark papers',disabled=False, indent=False, layout=Layout(width='25%'))\n",
    "    kWidget = widgets.IntSlider(value=10, description='k', max=50, min=1, layout=Layout(width='25%'))\n",
    "\n",
    "    button = widgets.Button(description=\"Search\")\n",
    "\n",
    "    display(VBox([HBox([kWidget, yearW, quarkW], layout=Layout(width='90%', justify_content='space-around')),\n",
    "        textW, button], layout=Layout(align_items='center')))\n",
    "\n",
    "    def on_button_clicked(b):\n",
    "        clear_output()\n",
    "        display(VBox([HBox([kWidget, yearW, quarkcovidW], layout=Layout(width='90%', justify_content='space-around')),\n",
    "            textW, button], layout=Layout(align_items='center')))        \n",
    "        relevant_articles(textW.value, kWidget.value, yearW.value[0], yearW.value[1], quarkW.value)\n",
    "\n",
    "    button.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_articles_for_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.gensim.prepare(lda, corpus_lda, dictionary, mds='tsne')\n",
    "panel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
